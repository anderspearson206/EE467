{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqMdQGWllsmv"
      },
      "source": [
        "# EE 467 Lab 7: Instruction Set Architecture (ISA) Identification of Program Binaries\n",
        "\n",
        "Welcome to Lab 7 of EE 467! Today we apply the machine learning techniques you’ve learned throughout the course to a practical cybersecurity task: Instruction Set Architecture (ISA) detection. We will use a dataset with **50k Base64-encoded binaries** downloaded from Praetorian’s “Machine Learning Binaries\" challenge web page [1]. Each of the encoded binary string in this dataset consists of 88 characters (66 bytes) on average and belongs to one of the following **twelve architecture types: avr, alphaev56, arm, m68k, mips, mipsel, powerpc, s390, sh4, sparc, x86_64, and xtensa**.\n",
        "\n",
        "We will use two different feature extration models, **byte-histogram+endianness features and byte-level TF-IDF features**, to extract features from the binaries. Using these features, we will **train and test SVM, Logisitic Regression, Decision Tree, and Random Forrest** classification algorithms. As in the previous labs, all algorithms are evaluated by **accuracy, precision, recall and F1-score**.\n",
        "\n",
        "## Preparation\n",
        "\n",
        "Like previous labs, we start by installing all dependencies needed for this lab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vNqXO4D_lsmx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\anders\\documents\\ee\\ee467\\.venv\\lib\\site-packages (2.4.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\anders\\documents\\ee\\ee467\\.venv\\lib\\site-packages (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\anders\\documents\\ee\\ee467\\.venv\\lib\\site-packages (1.8.0)\n",
            "Collecting termcolor\n",
            "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\anders\\documents\\ee\\ee467\\.venv\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\anders\\documents\\ee\\ee467\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Downloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: termcolor\n",
            "Successfully installed termcolor-3.3.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy scipy scikit-learn termcolor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJhrEt8llsmy"
      },
      "source": [
        "## Data Pre-processing\n",
        "\n",
        "We load the raw binary data and their labels from separate files. During loading we decode each Base64 string into bytes, preview a handful of samples in three representations (Base64, hex, and byte-level), deduplicate entries, and compute summary statistics over the full dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iXS4um1MrE1Q"
      },
      "outputs": [],
      "source": [
        "!tar -xJf binaries-dataset.tar.xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U4MVfdQlsmy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mBase64-encoded string:\u001b[0m\n",
            "\u001b[31mAC0GLQYCdQtKBuawYVrRiiFa0iYx/+AKYANm+XZR0iqEDGMrhAxhGGE7ISyEDGVTYChACyEthChAGEAbIANlag==\u001b[0m\n",
            "\u001b[1mHex-encoded string:\u001b[0m\n",
            "\u001b[34m002d062d0602750b4a06e6b0615ad18a215ad22631ffe00a600366f97651d22a840c632b840c6118613b212c840c65536028400b212d84284018401b2003656a\u001b[0m\n",
            "\u001b[1mByte level decomposition:\u001b[0m\n",
            "\u001b[32m00 2d 06 2d 06 02 75 0b 4a 06 e6 b0 61 5a d1 8a 21 5a d2 26 31 ff e0 0a 60 03 66 f9 76 51 d2 2a 84 0c 63 2b 84 0c 61 18 61 3b 21 2c 84 0c 65 53 60 28 40 0b 21 2d 84 28 40 18 40 1b 20 03 65 6a\u001b[0m\n",
            "\n",
            "\u001b[1mBase64-encoded string:\u001b[0m\n",
            "\u001b[31mAAAgkwAAkJMAAICTAACCgZOBkJMAAICTAACEgZWBkJMAAICTAACAkQAAkOCQkwAAgJMAAB+SH5Ifkh+SH5Ifkg==\u001b[0m\n",
            "\u001b[1mHex-encoded string:\u001b[0m\n",
            "\u001b[34m00002093000090930000809300008281938190930000809300008481958190930000809300008091000090e090930000809300001f921f921f921f921f921f92\u001b[0m\n",
            "\u001b[1mByte level decomposition:\u001b[0m\n",
            "\u001b[32m00 00 20 93 00 00 90 93 00 00 80 93 00 00 82 81 93 81 90 93 00 00 80 93 00 00 84 81 95 81 90 93 00 00 80 93 00 00 80 91 00 00 90 e0 90 93 00 00 80 93 00 00 1f 92 1f 92 1f 92 1f 92 1f 92 1f 92\u001b[0m\n",
            "\n",
            "\u001b[1mBase64-encoded string:\u001b[0m\n",
            "\u001b[31mAAAAAAAAAADAIAAoAqEAAMAgACkIfELAIAApCAwiwCAAKQgMEsAgACkIDALAIAApCMAgACkIkQAAwCAAqQjAIA==\u001b[0m\n",
            "\u001b[1mHex-encoded string:\u001b[0m\n",
            "\u001b[34m0000000000000000c020002802a10000c0200029087c42c0200029080c22c0200029080c12c0200029080c02c020002908c020002908910000c02000a908c020\u001b[0m\n",
            "\u001b[1mByte level decomposition:\u001b[0m\n",
            "\u001b[32m00 00 00 00 00 00 00 00 c0 20 00 28 02 a1 00 00 c0 20 00 29 08 7c 42 c0 20 00 29 08 0c 22 c0 20 00 29 08 0c 12 c0 20 00 29 08 0c 02 c0 20 00 29 08 c0 20 00 29 08 91 00 00 c0 20 00 a9 08 c0 20\u001b[0m\n",
            "\n",
            "\u001b[1mBase64-encoded string:\u001b[0m\n",
            "\u001b[31moOH+///rATCg4QMgoOE9MFvlAwBS4QEwoLMAMKCj/zAD4gMgoOE4MZ/lADCT5QIwA+BAIZ/lAgBT4QEwoAMAMA==\u001b[0m\n",
            "\u001b[1mHex-encoded string:\u001b[0m\n",
            "\u001b[34ma0e1feffffeb0130a0e10320a0e13d305be5030052e10130a0b30030a0a3ff3003e20320a0e138319fe5003093e5023003e040219fe5020053e10130a0030030\u001b[0m\n",
            "\u001b[1mByte level decomposition:\u001b[0m\n",
            "\u001b[32ma0 e1 fe ff ff eb 01 30 a0 e1 03 20 a0 e1 3d 30 5b e5 03 00 52 e1 01 30 a0 b3 00 30 a0 a3 ff 30 03 e2 03 20 a0 e1 38 31 9f e5 00 30 93 e5 02 30 03 e0 40 21 9f e5 02 00 53 e1 01 30 a0 03 00 30\u001b[0m\n",
            "\n",
            "\u001b[1mBase64-encoded string:\u001b[0m\n",
            "\u001b[31mAfA/RBAA4MMAAD0kAAAhoAEwIEACAOFDAAA9JAAAQbAAAD0kAABBoP3/PyChCUFA7P8/9AAAPSQAACEgBAAhKA==\u001b[0m\n",
            "\u001b[1mHex-encoded string:\u001b[0m\n",
            "\u001b[34m01f03f441000e0c300003d24000021a0013020400200e14300003d24000041b000003d24000041a0fdff3f20a1094140ecff3ff400003d240000212004002128\u001b[0m\n",
            "\u001b[1mByte level decomposition:\u001b[0m\n",
            "\u001b[32m01 f0 3f 44 10 00 e0 c3 00 00 3d 24 00 00 21 a0 01 30 20 40 02 00 e1 43 00 00 3d 24 00 00 41 b0 00 00 3d 24 00 00 41 a0 fd ff 3f 20 a1 09 41 40 ec ff 3f f4 00 00 3d 24 00 00 21 20 04 00 21 28\u001b[0m\n",
            "\n",
            "\u001b[1m[[ Dataset Statistics ]]\u001b[0m\n",
            "* Distinct labels: dict_keys(['sh4', 'avr', 'xtensa', 'arm', 'alphaev56', 's390', 'm68k', 'x86_64', 'mips', 'mipsel', 'sparc', 'powerpc'])\n",
            "* # of samples for all classes: Counter({'powerpc': 4204, 'mips': 4163, 'sparc': 4162, 'arm': 4160, 'm68k': 4146, 'x86_64': 4142, 'mipsel': 4141, 'xtensa': 4110, 'sh4': 4099, 's390': 4035, 'avr': 3921, 'alphaev56': 3838})\n",
            "* Min # of samples in class: 3838\n",
            "* # of samples in total: 49121\n",
            "* # of duplicates: 879\n"
          ]
        }
      ],
      "source": [
        "import base64\n",
        "from collections import Counter\n",
        "\n",
        "from termcolor import colored\n",
        "\n",
        "# Path of binaries and labels files\n",
        "BINARIES_PATH = \"./binaries-dataset/Base64EcncodedBinaries-50k.txt\"\n",
        "LABELS_PATH = \"./binaries-dataset/LabelsOfBinaries-50k.txt\"\n",
        "# Number of samples to display\n",
        "N_DISPLAY_SAMPLES = 5\n",
        "\n",
        "# Binaries and labels\n",
        "binaries = []\n",
        "binaries_set = set()\n",
        "raw_labels = []\n",
        "# Duplicated samples and displayed samples counts\n",
        "dup_count = 0\n",
        "display_count = 0\n",
        "\n",
        "# Decode each Base64-encoded binary into raw bytes, display a few samples in multiple\n",
        "# representations, and deduplicate entries before storing them.\n",
        "# Note: Each byte (8 bits, 256 possible values) is represented by 2 hex characters.\n",
        "with open(BINARIES_PATH) as binaries_file, open(LABELS_PATH) as binaries_label_file:\n",
        "    for line_tmp, label_tmp in zip(binaries_file, binaries_label_file):\n",
        "        # Remove EOL characters\n",
        "        line_eol_removed = line_tmp.rstrip()\n",
        "        raw_label = label_tmp.rstrip()\n",
        "\n",
        "        # [ ODO ]\n",
        "        # 1. Decode Base64-encoded binaries into byte strings\n",
        "        line_eol_removed_bytes = line_eol_removed.encode(\"utf-8\")\n",
        "        binary_sample = base64.b64decode(line_eol_removed_bytes)\n",
        "        \n",
        "\n",
        "        # Display the first N_DISPLAY_SAMPLES binaries in three representations\n",
        "        if display_count < N_DISPLAY_SAMPLES:\n",
        "            # [  ]\n",
        "            # 2. Encode byte strings as hex strings\n",
        "            hex_encoded = binary_sample.hex()\n",
        "            # 3. Rewrite encoded hex strings in byte-level granularities\n",
        "            #    (i.e. Separate byte data by spaces)\n",
        "            byte_level = \" \".join(hex_encoded[i:i+2] for i in range(0, len(hex_encoded), 2))\n",
        "\n",
        "            print(colored(\"Base64-encoded string:\", attrs=[\"bold\"]))\n",
        "            print(colored(line_eol_removed, \"red\"))\n",
        "\n",
        "            print(colored(\"Hex-encoded string:\", attrs=[\"bold\"]))\n",
        "            print(colored(hex_encoded, \"blue\"))\n",
        "\n",
        "            print(colored(\"Byte level decomposition:\", attrs=[\"bold\"]))\n",
        "            print(colored(byte_level, \"green\"))\n",
        "            print()\n",
        "\n",
        "            display_count += 1\n",
        "\n",
        "        # Skip duplicates; otherwise add sample to the dataset\n",
        "        if binary_sample not in binaries_set:\n",
        "            binaries.append(binary_sample)\n",
        "            binaries_set.add(binary_sample)\n",
        "            raw_labels.append(raw_label)\n",
        "        else:\n",
        "            dup_count += 1\n",
        "\n",
        "# Count labels\n",
        "labels_info = Counter(raw_labels)\n",
        "min_sample_size = min(labels_info.values())\n",
        "\n",
        "# Compute and print statistics\n",
        "print(colored(\"[[ Dataset Statistics ]]\", attrs=[\"bold\"]))\n",
        "\n",
        "print(\"* Distinct labels:\", labels_info.keys())\n",
        "print(\"* # of samples for all classes:\", labels_info)\n",
        "print(\"* Min # of samples in class:\", min(labels_info.values()))\n",
        "print(\"* # of samples in total:\", sum(labels_info.values()))\n",
        "print(\"* # of duplicates:\", dup_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLZ-vaUWlsmz"
      },
      "source": [
        "The statistics indicate that our dataset is largely balanced — all twelve ISA classes have similar sample counts, so no undersampling or oversampling is needed. We only need to map the string labels to integer indices for use with scikit-learn:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mbVQmxqllsmz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Map each unique string label to a consecutive integer (0, 1, 2, ...)\n",
        "raw_labels_to_labels = {label: i for i, label in enumerate(labels_info.keys())}\n",
        "\n",
        "# Build the integer label array used by all scikit-learn models below\n",
        "labels = np.array([raw_labels_to_labels[raw_label] for raw_label in raw_labels])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiFALuWelsmz"
      },
      "source": [
        "## Feature Extraction\n",
        "\n",
        "In this lab we are going to try three different feature extraction techniques:\n",
        "\n",
        "* Byte-Histogram and Endianness Features\n",
        "* Byte-level 1,2,3-Gram TF-IDF Features\n",
        "* Hex-level (4-bit) 1,2,3-Gram TF-IDF Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynzzqfXzlsmz"
      },
      "source": [
        "### Byte-Histogram and Endianness Features\n",
        "\n",
        "This feature extraction method was originally proposed in [2]. It builds two histograms from raw binary data:\n",
        "\n",
        "1. **Byte-value histogram**: Scans the binary one byte at a time, producing a 256-entry histogram (one bin per possible byte value, 0–255).\n",
        "2. **Endianness histogram**: Scans for four specific two-byte (word) patterns — `00 01`, `01 00`, `ff fe`, and `fe ff` — whose frequency hints at the endianness of the binary data.\n",
        "\n",
        "Both histograms are concatenated and then **normalized** by the total number of bytes in the binary, yielding a 260-dimensional feature vector that is comparable across binaries of different lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TH2e9bJGlsm0"
      },
      "outputs": [],
      "source": [
        "from collections.abc import Iterator\n",
        "\n",
        "# The four two-byte patterns whose presence signals endianness\n",
        "ENDIANNESS_WORDS = (b\"\\x00\\x01\", b\"\\x01\\x00\", b\"\\xff\\xfe\", b\"\\xfe\\xff\")\n",
        "\n",
        "def pairwise(bstr: bytes) -> Iterator[bytes]:\n",
        "    \"\"\"\n",
        "    Yield every consecutive overlapping byte pair in `bstr`.\n",
        "\n",
        "    Example: b\"uvwxyz\" -> b\"uv\", b\"vw\", b\"wx\", b\"xy\", b\"yz\"\n",
        "    \"\"\"\n",
        "    bstr_iter = iter(bstr)\n",
        "    char_a = next(bstr_iter)\n",
        "\n",
        "    for char_b in bstr_iter:\n",
        "        yield char_a + char_b\n",
        "        char_a = char_b\n",
        "\n",
        "def make_byte_hist_endian_feature(binary: bytes) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Return a 260-dim normalized feature vector for a single binary.\n",
        "\n",
        "    The vector is the concatenation of:\n",
        "      - a 256-entry byte-value histogram\n",
        "      - a 4-entry endianness-word histogram\n",
        "    divided element-wise by the binary's byte length.\n",
        "    \"\"\"\n",
        "    # Count occurrences of each byte value (0–255)\n",
        "    byte_hist = np.zeros(256, dtype=int)\n",
        "    for byte_data in binary:\n",
        "        byte_hist[byte_data] += 1\n",
        "\n",
        "    # Count occurrences of the four endianness-indicating word patterns\n",
        "    word_hist = np.zeros(len(ENDIANNESS_WORDS), dtype=int)\n",
        "    for word_data in pairwise(binary):\n",
        "        try:\n",
        "            word_idx = ENDIANNESS_WORDS.index(word_data)\n",
        "            word_hist[word_idx] += 1\n",
        "        except ValueError:\n",
        "            # Not an endianness marker; skip\n",
        "            pass\n",
        "\n",
        "    # Concatenate and normalize by binary length so features are scale-invariant\n",
        "    concat_hist = np.concatenate((byte_hist, word_hist))\n",
        "    return concat_hist / len(binary)\n",
        "\n",
        "feats_byte_hist_endian = np.stack([make_byte_hist_endian_feature(binary) for binary in binaries])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12wMcurNlsm0"
      },
      "source": [
        "### Byte-level 1,2,3-Gram TF-IDF Features\n",
        "\n",
        "This method generalizes the byte-histogram approach using TF-IDF weighting over byte n-grams (unigrams, bigrams, and trigrams). A few implementation notes:\n",
        "\n",
        "* We reuse scikit-learn's `TfidfVectorizer` with the character (`\"char\"`) analyzer so it treats each byte as a single character. Setting the encoding to `latin1` ensures every possible byte value (0–255) maps to a valid character without errors.\n",
        "* For trigrams, we cap the vocabulary with `max_features` to keep feature dimensionality (and training time) manageable.\n",
        "* Unigram+bigram and trigram matrices are merged in the feature dimension using `scipy.sparse.hstack`, matching the approach from homework 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eB8dhl5Nlsm0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Limit trigram vocabulary to keep memory and training time manageable\n",
        "TF_IDF_3_MAX_FEATS = 10000\n",
        "\n",
        "# [  ]\n",
        "# Extract Byte-level (1,2,3)-gram TF-IDF features using TfidfVectorizer\n",
        "# 1. Fit character (1,2)-gram TF-IDF model using `binaries` as training data, with `encoding` set to\n",
        "#    \"latin1\". Save transformed features as `feats_tf_idf_12`.\n",
        "tf_idf_12 = TfidfVectorizer(analyzer=\"char\", ngram_range=(1,2), encoding=\"latin1\")\n",
        "feats_tf_idf_12 = tf_idf_12.fit_transform(binaries)\n",
        "\n",
        "# 2. Fit character trigram TF-IDF model using `binaries` as training data, with `encoding` set to\n",
        "#    \"latin1\" and number of features limited by `TF_IDF_3_MAX_FEATS`. Save transformed features as\n",
        "#    `feats_tf_idf_3`.\n",
        "tf_idf_3 = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,3), encoding=\"latin1\", max_features=TF_IDF_3_MAX_FEATS)\n",
        "feats_tf_idf_3 = tf_idf_3.fit_transform(binaries)\n",
        "\n",
        "# 3. Concatenate `feats_tf_idf_12` and `feats_tf_idf_3` into `feats_tf_idf_123` using `scipy.sparse.hstack`.\n",
        "feats_tf_idf_123 = hstack((feats_tf_idf_12, feats_tf_idf_3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ubLldHglsm0"
      },
      "source": [
        "### Hex-level (4-bit) 1,2,3-Gram TF-IDF Features\n",
        "\n",
        "This method applies the same TF-IDF n-gram approach but operates on the **hex-encoded** representation of each binary. Because each hex character encodes four bits (half a byte), the resulting vocabulary captures finer-grained patterns than the byte-level approach, at the cost of doubling the sequence length. No `latin1` encoding override is needed here since all hex characters are standard ASCII.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2WOcyo3lsm0"
      },
      "outputs": [],
      "source": [
        "# [ TODO ]\n",
        "# Extract Hex-level (1,2,3)-gram TF-IDF features using TfidfVectorizer\n",
        "# 1. Hex-encode all binary data in `binaries` and save them as `binaries_hex`\n",
        "\n",
        "binaries_hex = base64.hex()\n",
        "# 2. Fit character (1,2,3)-gram TF-IDF model using `binaries_hex` as training data. Save transformed features\n",
        "#    as `feats_tf_idf_hex_123`.\n",
        "feats_tf_idf_hex_123 = NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSMLfdlplsm1"
      },
      "source": [
        "## Training and Testing Different ML Models\n",
        "\n",
        "Now that we have multiple feature representations, we evaluate each one across four classifiers: **Linear SVM**, **Logistic Regression**, **Decision Tree**, and **Random Forest**. To keep runtimes reasonable with a 50k-sample dataset, we use a stratified 20%/5% train/test split. We report accuracy, precision, recall, and F1-score, as well as the confusion matrix, for both the training and test sets.\n",
        "\n",
        "We start by implementing a shared `train_test_ml_models` function that encapsulates the complete train-evaluate loop so we can reuse it for each feature type:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFsJ3PG4lsm1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from contextlib import contextmanager\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Default random seed for reproducibility\n",
        "RNG_SEED = 42\n",
        "\n",
        "@contextmanager\n",
        "def timeit(action: str):\n",
        "    \"\"\"Context manager that prints elapsed wall-clock time for a code block.\"\"\"\n",
        "    start_time = time.time()\n",
        "    print(f\"Timing started for {action} ...\")\n",
        "\n",
        "    yield\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Timing ended for {action}. Elapsed time: {elapsed_time:.2f}s\")\n",
        "\n",
        "def train_test_ml_models(feats: np.ndarray, labels: np.ndarray, train_size: float = 0.2, test_size: float = 0.05,\n",
        "    rng_seed_data: int = RNG_SEED, rng_seed: int = RNG_SEED):\n",
        "    \"\"\"\n",
        "    Train and evaluate four classifiers on the given features and labels.\n",
        "\n",
        "    Uses a stratified train/test split to preserve class balance, then reports\n",
        "    confusion matrices and full classification reports for both subsets.\n",
        "    \"\"\"\n",
        "    # Stratified split keeps class proportions consistent across train and test sets\n",
        "    binaries_train, binaries_test, labels_train, labels_test = train_test_split(\n",
        "        feats, labels, train_size=train_size, test_size=test_size, stratify=labels, random_state=rng_seed_data\n",
        "    )\n",
        "\n",
        "    # List of ML models to train and evaluate\n",
        "    ml_models = {\n",
        "        \"SVM\": LinearSVC(max_iter=200, random_state=rng_seed),\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=200, random_state=rng_seed),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(random_state=rng_seed),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=rng_seed)\n",
        "    }\n",
        "\n",
        "    for model_name, model in ml_models.items():\n",
        "        print(colored(f\"[[ Training and Evaluation for {model_name} ]]\", attrs=[\"bold\"]))\n",
        "        print(colored(\"[ Training ]\", attrs=[\"bold\"]))\n",
        "\n",
        "        with timeit(action=f\"training of {model_name}\"):\n",
        "            model.fit(binaries_train, labels_train)\n",
        "\n",
        "        # Compute and show metrics for training and testing sets\n",
        "        for subset_name, binaries_eval, labels_eval in (\n",
        "            (\"Training\", binaries_train, labels_train),\n",
        "            (\"Testing\", binaries_test, labels_test)\n",
        "        ):\n",
        "            preds = model.predict(binaries_eval)\n",
        "\n",
        "            print(colored(f\"[ {subset_name} Set ]\", attrs=[\"bold\"]))\n",
        "\n",
        "            print(\"Confusion matrix:\")\n",
        "            print(confusion_matrix(labels_eval, preds))\n",
        "            print()\n",
        "\n",
        "            print(\"Classification report:\")\n",
        "            print(classification_report(labels_eval, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kdVK-4olsm1"
      },
      "source": [
        "With the training and evaluation routine implemented, let's first run it on the **Byte-Histogram and Endianness Features**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RB0Pe4emlsm1"
      },
      "outputs": [],
      "source": [
        "train_test_ml_models(feats_byte_hist_endian, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R69HkAMQlsm1"
      },
      "source": [
        "... and for **Byte-level 1,2,3-Gram TF-IDF Features**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXdPs8-Blsm1"
      },
      "outputs": [],
      "source": [
        "train_test_ml_models(feats_tf_idf_123, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vio4XsB-lsm1"
      },
      "source": [
        "Let's check the shape of the byte-level TF-IDF feature matrix before deciding on dimensionality reduction:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlvF3gWdlsm1"
      },
      "outputs": [],
      "source": [
        "feats_tf_idf_123.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lj54-uSlsm1"
      },
      "source": [
        "We'll find out that for **Byte-level 1,2,3-Gram TF-IDF Features**, there are more than 32,000 feature dimensions per sample. To ease the computational burden, we apply `KernelPCA` with an RBF kernel to reduce dimensionality to 300 components. Because `KernelPCA` does not natively support sparse input, we first fit it on a randomly-selected 20% subset of the data to reduce memory usage, then transform the full dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAgZS6DFlsm1"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "KERNEL_PCA_DIMS = 300\n",
        "\n",
        "# KernelPCA is fitted on a 20% stratified subset to reduce memory and runtime.\n",
        "# The full dataset is then transformed using the fitted model.\n",
        "feats_tf_idf_123_kpca_train, _ = train_test_split(\n",
        "    feats_tf_idf_123, train_size=0.2, stratify=labels, random_state=RNG_SEED\n",
        ")\n",
        "\n",
        "with timeit(\"Kernel PCA fitting\"):\n",
        "    kpca_tf_idf_123 = KernelPCA(n_components=KERNEL_PCA_DIMS, kernel=\"rbf\", random_state=RNG_SEED)\n",
        "    kpca_tf_idf_123.fit(feats_tf_idf_123_kpca_train)\n",
        "\n",
        "with timeit(\"Kernel PCA dimensionality reduction\"):\n",
        "    feats_tf_idf_123_kpca = kpca_tf_idf_123.transform(feats_tf_idf_123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnrIYke8lsm1"
      },
      "source": [
        "We can now run training and evaluation using the dimensionality-reduced (300-component) byte-level TF-IDF features:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFoT9ISplsm1"
      },
      "outputs": [],
      "source": [
        "train_test_ml_models(feats_tf_idf_123_kpca, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKRahyeAlsm1"
      },
      "source": [
        "Finally, let's run training and evaluation on the **Hex-level 1,2,3-Gram TF-IDF Features**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVXvYbLtlsm1"
      },
      "outputs": [],
      "source": [
        "train_test_ml_models(feats_tf_idf_hex_123, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBf9TFdblsm2"
      },
      "source": [
        "## References\n",
        "\n",
        "1. Tech challenge: Machine learning binaries,” Feb 2021. [Online]. Available: https://www.praetorian.com/challenges/machine-learning-challenge/#how-to-play\n",
        "2. J. Clemens, “Automatic classification of object code using machine learning,” Digital Investigation, vol. 14, pp. S156–S162, 2015.\n",
        "3. D. Sahabandu, J. S. Mertoguno and R. Poovendran, \"A Natural Language Processing Approach for Instruction Set Architecture Identification,\" in IEEE Transactions on Information Forensics and Security, vol. 18, pp. 4086-4099, 2023, doi: 10.1109/TIFS.2023.3288456.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
